{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae415ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiser17/anaconda3/envs/patra2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from visualize_IG import *\n",
    "from utils_IG import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from smooth_grad.gradients import VanillaGrad, SmoothGrad, GuidedBackpropGrad, GuidedBackpropSmoothGrad\n",
    "from smooth_grad.image_utils import preprocess_image, save_as_gray_image\n",
    "from smooth_grad.labels import IMAGENET_LABELS\n",
    "# from GuideBP import Guided_backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c596755",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))\n",
    "from cartoonX import CartoonX \n",
    "from pixelRDE import PixelRDE\n",
    "from utils_IG import calculate_outputs_and_gradients, generate_entrie_images\n",
    "from Integrated_grad import random_baseline_integrated_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad122d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_LIST = tuple(open(os.path.join(sys.path[0], \"imagenet_labels.txt\")).read().split('\\n'))\n",
    "LABEL_LIST = [x.replace('{',\"\").replace('\\'',\"\").replace(',',\"\").replace('-',\" \").replace('_',\" \") for x in LABEL_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa51ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('random_images/')\n",
    "imgdir='random_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125b8a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiser17/anaconda3/envs/patra2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kaiser17/anaconda3/envs/patra2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get classifier to explain\n",
    "model = models.mobilenet_v3_small(pretrained=True).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb2b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(sys.path[0], \"hparams.yaml\")) as f:\n",
    "    HPARAMS_CARTOONX = yaml.load(f, Loader=yaml.FullLoader)[\"CartoonX\"]\n",
    "\n",
    "with open(os.path.join(sys.path[0], \"hparams.yaml\")) as f:\n",
    "    HPARAMS_PIXEL_RDE = yaml.load(f, Loader=yaml.FullLoader)[\"PixelRDE\"]\n",
    "\n",
    "# Initialize wavelet RDE and pixel RDE\n",
    "cartoonX = CartoonX(model=model, device=device, **HPARAMS_CARTOONX)\n",
    "pixelRDE = PixelRDE(model=model, device=device, **HPARAMS_PIXEL_RDE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a435fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_largest_index_argsort(a, k):\n",
    "    idx = np.argsort(a.ravel())[:-k-1:-1]\n",
    "    return np.column_stack(np.unravel_index(idx, a.shape))\n",
    "\n",
    "def createmaskedimage(img,grad,percent,noise):\n",
    "    #grad:L X B\n",
    "    num_pixels=int(grad.shape[0]*grad.shape[1]*percent)\n",
    "    mask=np.zeros((grad.shape[0],grad.shape[1]))\n",
    "    topk_idx=k_largest_index_argsort(grad,num_pixels)\n",
    "    mask[topk_idx[:,0],topk_idx[:,1]]=1\n",
    "    mask=torch.tensor(mask).unsqueeze(0).unsqueeze(0).to(device)#1 X 1 X L X B\n",
    "    noise=torch.randn(img.shape)\n",
    "    new_img=img*mask+torch.clip(noise.to(device)*(1-mask),0,1)\n",
    "    return(mask,new_img)\n",
    "\n",
    "def save_as_gray_image(img, filename, percentile=99):#smoothgrad\n",
    "    img_2d = np.sum(img, axis=0)\n",
    "    span = abs(np.percentile(img_2d, percentile))\n",
    "    vmin = -span\n",
    "    vmax = span\n",
    "    img_2d = np.clip((img_2d - vmin) / (vmax - vmin), -1, 1)\n",
    "    cv2.imwrite(filename, img_2d * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64980af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "percent_list=list(np.array(list(range(0,101)))/100)\n",
    "full_array=torch.zeros((len(files),len(percent_list)))\n",
    "full_array_smooth=torch.zeros((len(files),len(percent_list)))\n",
    "full_array_cartoon=torch.zeros((len(files),len(percent_list)))\n",
    "full_array_randomwavelet=torch.zeros((len(files),len(percent_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f71a6",
   "metadata": {},
   "source": [
    "# Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e1ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Integrated Gradients\n",
    "\n",
    "for i,fname in enumerate(files):\n",
    "    print(f\"Processing file: {fname}\")\n",
    "    # Get image and transform to tensor    \n",
    "    img = cv2.imread(os.path.join(imgdir, fname))\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = img.astype(np.float32) \n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    # calculate the gradient and the label index\n",
    "    gradients, label_index = calculate_outputs_and_gradients([img], model, None, True)\n",
    "    gradients = np.transpose(gradients[0], (1, 2, 0))\n",
    "\n",
    "    attributions = random_baseline_integrated_gradients(img, model, label_index, calculate_outputs_and_gradients, \\\n",
    "                                                        steps=100, num_random_trials=10, cuda=True)\n",
    "\n",
    "    \n",
    "    input_img = pre_processing(img, True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j,percent in enumerate(percent_list):\n",
    "            noise=torch.randn(input_img.shape)\n",
    "            mask_,masked_img=createmaskedimage(input_img,attributions.mean(2),percent,noise)\n",
    "\n",
    "            full_array[i,j]=((F.softmax(model(input_img),1)-F.softmax(model(masked_img.float()),1))**2).sum()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('IntegG.npy', full_array)\n",
    "# xxx=np.load('IntegG.npy')\n",
    "# plt.plot(xxx.mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af807964",
   "metadata": {},
   "source": [
    "## Smooth Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc14fc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SmoothGrad\n",
    "for i,fname in enumerate(files):\n",
    "    print(f\"Processing file: {fname}\")\n",
    "    # Get image and transform to tensor    \n",
    "    img = cv2.imread(os.path.join(imgdir, fname))\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = img.astype(np.float32) \n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    input_img = pre_processing(img, True)\n",
    "    \n",
    "    output = model(input_img)\n",
    "    pred_index = np.argmax(output.data.cpu().numpy())\n",
    "    smooth_grad = SmoothGrad(\n",
    "        pretrained_model=model,\n",
    "        cuda=True,\n",
    "        n_samples=1000,\n",
    "        magnitude=True)\n",
    "    smooth_saliency = smooth_grad(input_img, index=None)\n",
    "\n",
    "    input_img = pre_processing(img, True)  \n",
    "    with torch.no_grad():\n",
    "        for j,percent in enumerate(percent_list):\n",
    "            noise=torch.randn(input_img.shape)\n",
    "            mask_,masked_img=createmaskedimage(input_img,smooth_saliency.sum(0),percent,noise)\n",
    "\n",
    "            full_array_smooth[i,j]=((F.softmax(model(input_img),1)-F.softmax(model(masked_img.float()),1))**2).sum()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd383c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('SmoothG.npy', full_array_smooth)\n",
    "# xxx=np.load('SmoothG.npy')\n",
    "# plt.plot(xxx.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b34c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525db95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d58324c",
   "metadata": {},
   "source": [
    "## Guided Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a8b73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Guided BackProp\n",
    "full_array_GBP=torch.zeros((len(files),len(percent_list)))\n",
    "for i,fname in enumerate(files):\n",
    "    print(f\"Processing file: {fname}\")\n",
    "    # Get image and transform to tensor    \n",
    "    img = cv2.imread(os.path.join(imgdir, fname))\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = img.astype(np.float32) \n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    input_img = pre_processing(img, True)\n",
    "    \n",
    "    output = model(input_img)\n",
    "    pred_index = np.argmax(output.data.cpu().numpy())\n",
    "\n",
    "    guided_grad = GuidedBackpropGrad(\n",
    "        pretrained_model=model, cuda=True)\n",
    "    \n",
    "    guided_saliency = guided_grad(input_img, index=None)\n",
    "#     print(\"shape-\",guided_saliency.shape)\n",
    "    input_img = pre_processing(img, True)   \n",
    "    with torch.no_grad():\n",
    "        for j,percent in enumerate(percent_list):\n",
    "            noise=torch.randn(input_img.shape)\n",
    "            mask_,masked_img=createmaskedimage(input_img,guided_saliency.sum(0),percent,noise)\n",
    "\n",
    "            full_array_GBP[i,j]=((F.softmax(model(input_img),1)-F.softmax(model(masked_img.float()),1))**2).sum()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1303a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GuidedBP.npy', full_array_GBP)\n",
    "# xxx=np.load('GuidedBP.npy')\n",
    "# plt.plot(xxx.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82415d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marker(mask,percent):\n",
    "    if percent==0:\n",
    "        return 99999999\n",
    "    elif percent==1:\n",
    "#         print(\"right\")\n",
    "        return -9999999\n",
    "    mask_open=[mask[0].reshape(-1)]\n",
    "    for i in range(len(mask[1])):\n",
    "        mask_open.append(mask[1][i].reshape(-1))\n",
    "    final_mask_open=torch.cat(mask_open,0).detach().cpu().numpy()\n",
    "    final_mask_open=final_mask_open.reshape(-1,1) # 60000 X 1\n",
    "    noise_symmet=torch.randn(final_mask_open.shape).numpy()*0.000000001\n",
    "    topk_idx=k_largest_index_argsort(noise_symmet+final_mask_open,int(final_mask_open.shape[0]*percent))\n",
    "    return(final_mask_open[topk_idx[-1][0],topk_idx[-1][1]])\n",
    "\n",
    "def createmaskedwavelt(wavelet,grad_mask,mark):\n",
    "    \n",
    "\n",
    "#     print(\"look-\",wavelet.shape,mask.shape)\n",
    "    noise=(torch.randn(wavelet.shape).to(device)*(torch.std(wavelet)).to(device))+torch.mean(wavelet).to(device)\n",
    "    new_wavelet=wavelet*(grad_mask>=mark)+noise.to(device)*((grad_mask<mark))\n",
    "    return(mask,new_wavelet.float())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ae667",
   "metadata": {},
   "source": [
    "## CartoonX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartoon\n",
    "for i,fname in enumerate(files):\n",
    "    print(f\"Processing file: {fname}\")\n",
    "    x = Image.open(os.path.join(imgdir, fname))\n",
    "    x = transforms.ToTensor()(x)\n",
    "    x = transforms.Resize(size=(256,256))(x)\n",
    "    x = x.to(device).unsqueeze(0)\n",
    "    input_img=x.detach()\n",
    "    # classifiy image\n",
    "#     print(\"XXX\",x.shape)\n",
    "    output = model(x)\n",
    "    pred = nn.Softmax(dim=1)(output).max(1)[1].item() \n",
    "\n",
    "    # set CartoonX hyperparameters\n",
    "    HPARAMS ={\"wave\": \"db3\", \"mode\": \"zero\", \"J\": 5,\n",
    "              \"l1lambda\": 10, \"step_size\": 1e-3,\n",
    "              \"num_steps\": 100,  \"batch_size\": 16,\n",
    "              \"distortion_measure\": \"label\"} \n",
    "\n",
    "    # get Cartoon RDE \n",
    "    cartoonX = CartoonX(model=model, device=device,return_mask=True, **HPARAMS)\n",
    "    explanation,mask = cartoonX(x, pred)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         print(percent_list)\n",
    "        for j,percent in enumerate(percent_list):\n",
    "            yl, yh = cartoonX.forward_dwt(x) \n",
    "            markz=marker(mask,percent)\n",
    "            _,obf_yl = createmaskedwavelt(yl,mask[0],markz)\n",
    "            obf_yh = []\n",
    "            for count, y in enumerate(yh):\n",
    "\n",
    "                obf_yh.append(createmaskedwavelt(yh[count],mask[1][count],markz)[1])\n",
    "            z = cartoonX.inverse_dwt((obf_yl,obf_yh))\n",
    "            masked_img = z.clamp(0,1) \n",
    "            full_array_cartoon[i,j]=((F.softmax(model(input_img))-F.softmax(model(masked_img.float())))**2).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7382a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cartoon.npy', full_array_cartoon)\n",
    "# xxx=np.load('cartoon.npy')\n",
    "# plt.plot(xxx.mean(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4af88",
   "metadata": {},
   "source": [
    "## Random wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random wavelet\n",
    "\n",
    "\n",
    "\n",
    "for i,fname in enumerate(files):\n",
    "#     print(f\"Processing file: {fname}\")\n",
    "    x = Image.open(os.path.join(imgdir, fname))\n",
    "    x = transforms.ToTensor()(x)\n",
    "    x = transforms.Resize(size=(256,256))(x)\n",
    "    x = x.to(device).unsqueeze(0)\n",
    "    input_img=x\n",
    "\n",
    "    output = model(x)\n",
    "    pred = nn.Softmax(dim=1)(output).max(1)[1].item() \n",
    "\n",
    "    # set CartoonX hyperparameters\n",
    "    HPARAMS ={\"wave\": \"db3\", \"mode\": \"zero\", \"J\": 5,\n",
    "              \"l1lambda\": 10, \"step_size\": 1e-3,\n",
    "              \"num_steps\": 100,  \"batch_size\": 16,\n",
    "              \"distortion_measure\": \"label\"} \n",
    "\n",
    "    # get Cartoon RDE \n",
    "    cartoonX = CartoonX(model=model, device=device,return_mask=True, **HPARAMS)\n",
    "    explanation,mask = cartoonX(x, pred)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mask[0]=torch.randn(mask[0].shape).to(device)\n",
    "        for dd in range(len(mask[1])):\n",
    "            mask[1][dd]=torch.randn(mask[1][dd].shape).to(device)\n",
    "        for j,percent in enumerate(percent_list):\n",
    "            yl, yh = cartoonX.forward_dwt(x) \n",
    "            markz=marker(mask,percent)\n",
    "            _,obf_yl = createmaskedwavelt(yl,mask[0],markz)\n",
    "            obf_yh = []\n",
    "            for count, y in enumerate(yh):\n",
    "\n",
    "                obf_yh.append(createmaskedwavelt(yh[count],mask[1][count],markz)[1])\n",
    "            z = cartoonX.inverse_dwt((obf_yl,obf_yh))\n",
    "            masked_img = z.clamp(0,1) \n",
    "            full_array_randomwavelet[i,j]=((F.softmax(model(input_img))-F.softmax(model(masked_img.float())))**2).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('randomwavelet.npy', full_array_randomwavelet)\n",
    "# xxx=np.load('randomwavelet.npy')\n",
    "# plt.plot(xxx.mean(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1937ce",
   "metadata": {},
   "source": [
    "## Random Pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f97db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Pixel\n",
    "full_array_randompixel=torch.zeros((len(files),len(percent_list)))\n",
    "\n",
    "\n",
    "for i,fname in enumerate(files):\n",
    "    print(f\"Processing file: {fname}\")\n",
    "    # Get image and transform to tensor    \n",
    "    img = cv2.imread(os.path.join(imgdir, fname))\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = img.astype(np.float32) \n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    input_img = pre_processing(img, True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j,percent in enumerate(percent_list):\n",
    "            noise=torch.randn(input_img.shape)\n",
    "            mask_,masked_img=createmaskedimage(input_img,np.random.randn(256,256),percent,noise)\n",
    "            full_array_randompixel[i,j]=((F.softmax(model(input_img),1)-F.softmax(model(masked_img.float()),1))**2).sum()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d38559",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('randompixel.npy', full_array_randompixel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f4151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a620e50",
   "metadata": {},
   "source": [
    "## Pixel RDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49387cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS_PIXEL_RDE['num_steps']=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pixel RDE\n",
    "full_array_pixelRDE=torch.zeros((len(files),len(percent_list)))\n",
    "\n",
    "for i,fname in enumerate(files):\n",
    "    print(f\"Processing file: {fname}\")\n",
    "    x = Image.open(os.path.join(imgdir, fname))\n",
    "    x = transforms.ToTensor()(x)\n",
    "    x = transforms.Resize(size=(256,256))(x)\n",
    "    x = x.to(device).unsqueeze(0)\n",
    "    \n",
    "    input_img = Image.open(os.path.join(imgdir, fname))\n",
    "    input_img = transforms.ToTensor()(input_img)\n",
    "    input_img = transforms.Resize(size=(256,256))(input_img)\n",
    "    input_img = input_img.to(device).unsqueeze(0)\n",
    "    \n",
    "\n",
    "    output = model(x)\n",
    "    pred = nn.Softmax(dim=1)(output).max(1)[1].item() \n",
    "    pixelRDE = PixelRDE(model=model, device=device, **HPARAMS_PIXEL_RDE)\n",
    "    mask = pixelRDE(x, pred)\n",
    "    \n",
    "    mask=mask.cpu().numpy()\n",
    "    with torch.no_grad():\n",
    "        for j,percent in enumerate(percent_list):\n",
    "            noise=torch.randn(input_img.shape)\n",
    "            mask_,masked_img=createmaskedimage(input_img,mask,percent,noise)\n",
    "            full_array_pixelRDE[i,j]=((F.softmax(model(input_img))-F.softmax(model(masked_img.float())))**2).sum()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pixelrde.npy', full_array_pixelRDE)\n",
    "# xxx=np.load('pixelrde.npy')\n",
    "# plt.plot(xxx.mean(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b58e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
